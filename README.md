# PracticalD2T Hackathon

## LLM access
- our API
- Google Colab

## Main Task: ☀️ Weather Forecast Generation

Your task is to build a system for automatically **generating weather forecasts**. 

The system will have access to [OpenWeather API](https://openweathermap.org/api) which provides access to up-to-date weather data from anywhere around the world.

The forecasts should be:
- automatically generated by **open-source** code (+ open language models)
- **human-readable**
- **accurate** with respect to the input data
- **concise** (covering important insights only)
- complete, captivating, insightful ...

### OpenWeather API
For starters, you can find a set of Python scripts for interacting with the [OpenWeather API](https://openweathermap.org/api) in the `scripts/openweather` directory.

You can use each script in either online or offline mode:
- for **online** use, use the argument `-l` / `--location` which is a free-form string describing an address anywhere around the world, e.g. "Prague, Czechia". (The location gets automatically geocoded to GPS coordinates using the [Nominatim](https://nominatim.org) library.)
- for **offline** use, use the argument `-j`/`--json` which is a path to the downloaded JSON file. (This is the preffered form of using the scripts, see the *API key* section).

The script outputs the response a nicely formatted chart, using only a subset of data. Feel free to go through the [docs](https://openweathermap.org/api) and extract the data you actually need!

On [free tier](https://openweathermap.org/price), OpenWeather API provides access to the following data:
- [current weather](https://openweathermap.org/current)
- [5-day forecast (with 3-hour resolution)](https://openweathermap.org/forecast5)
- [air pollution](https://openweathermap.org/api/air-pollution)



#### API key
You will be given to a shared API key. The key is on free tier which is **limited to 1000 requests per day**. 

Please, use it **only to pre-download several data files** for the locations and dates of your choice (up to 1000 requests / # teams) and use the local files.

You can also create your own free account(s) to get extra requests.


#### Data files
In the `data` folder, you can find various pre-downloaded files for various locations which you can use for 

### Task #1: Describe the current weather in the city
Pi


### Task #2: Generate a 5-day forecast

### Task #3: Generate a weather forecast in the local language


### Evaluation
- human evaluation
- automatic hallucination metrics
- weathergov?


## Alternative Tasks

### Our World In Data

The server [Our World In Data](https://ourworldindata.org/) provides [open data](https://github.com/owid/owid-datasets/tree/master/datasets) in machine-readable format on many interesting topics, including [climate change](https://ourworldindata.org/charts#climate-change), [COVID-19](https://ourworldindata.org/charts#covid-19), or [Annual scholarly publications on AI](https://ourworldindata.org/grapher/annual-scholarly-publications-on-artificial-intelligence).

You can download each data source underlying the chart in a CSV format.

The basic task is: "automatically generate a caption for the chart of your choice". 

But of course, you can be more creative. What about "report interesting trends" or "describe connections across charts"?

### Report on LLM data processing capabilities

This task is a bit more investigative. The goal is to compile a table with capabilities of different LLMs to "digest" different kinds of structured inputs.

 Which models can work from scratch with CSV, JSONs, etc.? And do the accept any input, or are there any cases which cause problems (abbreviations in headers, missing headers, deep nesting, ...)?

The ultimate goal is then to find cases where the LLMs **cannot** successfully process structured data – the output either contains hallucinations or does not make sense at all.

The inputs can be either handcrafted or syntethically generated, the outputs should generally contain descriptions / summaries of the data. Your task is to evaluate and annotate the outputs.

### Explainable data-to-text generation
This task is more linguistically-oriented, and can be in principle combined with any of the previous tasks. The goal is to generate **intermediate representations** of the output text that would help with controlling the output and explain the reasoning process of the model.

The intermediate representations can be on the different levels of abstraction: predicate-argument, deep syntax, surface syntax, etc. (see some of the intermediate representations [here](https://aclanthology.org/W19-8659/)).

The representations should be faithful both to the input data and to the output text, providing an additional level of control over the model.

Besides the inputs from the previous tasks, you can also use some of the more standard data-to-text datasets such as [WebNLG](https://huggingface.co/datasets/web_nlg), [E2E](https://huggingface.co/datasets/GEM/e2e_nlg), or [SportSett:Basketball](https://github.com/nlgcat/sport_sett_basketball). For quick visualization and easy loading of these (and many other) data-to-text datasets you can use [Tabgenie](https://github.com/kasnerz/tabgenie) ([live demo](http://quest.ms.mff.cuni.cz/rel2text/tabgenie)).


*(Credits to Simon Mille for coming up with the last two tasks.)*